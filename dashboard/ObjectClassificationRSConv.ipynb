{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "import pyvista as pv\n",
    "import torch\n",
    "\n",
    "os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
    "os.environ['DISPLAY'] = ':99'\n",
    "os.environ['PYVISTA_OFF_SCREEN'] = 'True'\n",
    "os.environ['PYVISTA_USE_PANEL'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"40%\" src=\"https://raw.githubusercontent.com/nicolas-chaulet/torch-points3d/master/docs/logo.png\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiying objects with Relation-Shape CNN\n",
    "This notebook explains how Torch Points3D can be used to solve an object classification task. We will use [ModelNet](https://modelnet.cs.princeton.edu/) as our dataset and [Relation-Shape CNN](https://github.com/Yochengliu/Relation-Shape-CNN) as our model architecture. The notebook covers the following aspects of Torch Points3D:\n",
    "\n",
    "1. Create a dataset with a data augmentation pipeline\n",
    "2. Instantiate a pre configured model\n",
    "3. Setup the data loaders\n",
    "4. Run a training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "We will use Torch Points3D to download and create the dataset. It automatically downloads a pre sampled version of ModelNet (with 10,000 points per object) and the data will be stored under the `ROOT/data/modelnet` directory. Creating the raw dataset is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.classification.modelnet import SampledModelNet\n",
    "import torch_points3d.core.data_transform as T3D\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "dataroot = os.path.join(DIR, \"data/modelnet\")\n",
    "pre_transform = T.Compose([T.NormalizeScale(), T3D.GridSampling(0.01)])\n",
    "dataset = SampledModelNet(dataroot, name='10', train=True, transform=None,\n",
    "                 pre_transform=pre_transform, pre_filter=None)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is a CAD model of an object (bathtub, toilet, bed, etc...), the `pos` attribute contains the coordinates of points sampled on each face of the mesh while `norm` contains the normal vector. You will notice that we have applied two pre transforms to our dataset:\n",
    "\n",
    "- `NormalizeScale` normalises the scale of each object\n",
    "- `GridSampling` does a voxel downsizing of each point cloud with a resolution of 0.01 (in normalized scale). This ensures that we don't have really dense areas.\n",
    "\n",
    "Let's visualise some of those examples before going any further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [10,1000,3000]\n",
    "p = pv.Plotter(notebook=True,shape=(1, len(samples)),window_size=[1024,412])\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    p.subplot(0, i)\n",
    "    sample = dataset[samples[i]].pos.numpy()\n",
    "    point_cloud = pv.PolyData(sample)\n",
    "    point_cloud['y'] = sample[:,1]\n",
    "    p.add_points(point_cloud)\n",
    "    p.camera_position = [-1,5, -10]\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "As mentioned in the introduction we will use Relation-Shape CNN as our backbone architecture for the model classifier. Let's build our model by using Torch Points3D API (you can ignore the error, it comes from a third party library and all works just fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.applications.rsconv import RSConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSConvCLassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.encoder = RSConv(\"encoder\", input_nc=3,output_nc = 10, num_layers=4)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def get_output(self):\n",
    "        \"\"\" This is needed by the tracker to get access to the ouputs of the network\"\"\"\n",
    "        return self.output\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\" Needed by the tracker in order to access ground truth labels\"\"\"\n",
    "        return self.labels\n",
    "    \n",
    "    def get_current_losses(self):\n",
    "        \"\"\" Entry point for the tracker to grab the loss \"\"\"\n",
    "        return {\"loss_class\": float(self.loss_class)}\n",
    "    \n",
    "    def forward(self, data):\n",
    "        data_out = self.encoder(data)\n",
    "        \n",
    "        # Set variables for the tracker and backward pass\n",
    "        self.output = self.log_softmax(data_out.x.squeeze())\n",
    "        self.labels = data.y.squeeze()\n",
    "        self.loss_class = torch.nn.functional.nll_loss(self.output, self.labels)\n",
    "    \n",
    "    def backward(self):\n",
    "         self.loss_class.backward()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RSConvCLassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data loaders\n",
    "In this section, we will first build  a data loader using only PyTorch Geometric methods, and then we will see how one can use our dataset wrapper to assemble a dataset ready for training with its test and train data loader configured.\n",
    "\n",
    "Let's start with a vanillia data loader. There are two ways to assemble batches depending on the model that is used. In that case we will want dense batches where samples are concatenated along the first dimension, just like for batches of images. It is very important to ensure that each sample has got the same number of points, this can be achieved by applying a `FixedPoints` transform to the dataset. here we will use 2048 points per object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.batch import SimpleBatch\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "transform = T.FixedPoints(2048)\n",
    "dataset = SampledModelNet(dataroot, name='10', train=True, transform=transform,\n",
    "                 pre_transform=pre_transform, pre_filter=None)\n",
    "\n",
    "collate_function = lambda datalist: SimpleBatch.from_data_list(datalist)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,  \n",
    "    collate_fn=collate_function\n",
    ")\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were using a model that can handle packed data we would not need the `FixedPoints` transform at all and each object could have a different number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "dataset = SampledModelNet(dataroot, name='10', train=True, transform=None,\n",
    "                 pre_transform=pre_transform, pre_filter=None)\n",
    "\n",
    "collate_function = lambda datalist: Batch.from_data_list(datalist)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,  \n",
    "    collate_fn=collate_function\n",
    ")\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a higher level interface that builds the train and test data loaders for you based on which model you use. We use an OmegaConf config to instantiate it (or any object with a similar interface). The config must contain the arguments required by the dataset. We can also include the pre transforms, test transforms and train transforms. In our case, the config will be as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config = \"\"\"\n",
    "task: classification\n",
    "class: modelnet.ModelNetDataset\n",
    "name: modelnet\n",
    "dataroot: {}\n",
    "number: 10\n",
    "pre_transforms:\n",
    "    - transform: NormalizeScale\n",
    "    - transform: GridSampling\n",
    "      lparams: [0.02]\n",
    "train_transforms:\n",
    "    - transform: FixedPoints\n",
    "      lparams: [2048]\n",
    "    - transform: RandomNoise\n",
    "    - transform: RandomRotate\n",
    "      params:\n",
    "        degrees: 180\n",
    "        axis: 2\n",
    "    - transform: AddFeatsByKeys\n",
    "      params:\n",
    "        feat_names: [norm]\n",
    "        list_add_to_x: [True]\n",
    "        delete_feats: [True]\n",
    "test_transforms:\n",
    "    - transform: FixedPoints\n",
    "      lparams: [2048]\n",
    "    - transform: AddFeatsByKeys\n",
    "      params:\n",
    "        feat_names: [norm]\n",
    "        list_add_to_x: [True]\n",
    "        delete_feats: [True]\n",
    "\"\"\".format(os.path.join(DIR, \"data\"))\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "params = OmegaConf.create(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.classification.modelnet import ModelNetDataset\n",
    "dataset = ModelNetDataset(params)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the train and test transforms have been set properly. The batch size is still not set since the data loaders have not been instantiaded yet, let's do that now and we will have a dataset ready for training. We can also instantiate the tracker directly from the dataset which reduces potential risks in using the wrong metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.create_dataloaders(\n",
    "    model, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    precompute_multi_scale=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataset.test_dataloaders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = dataset.get_tracker(False, False)\n",
    "tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!\n",
    "We can now start our training loop, we will use the Adam optimizer with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq\n",
    "import time\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tracker.reset(\"train\")\n",
    "    train_loader = dataset.train_dataloader\n",
    "    iter_data_time = time.time()\n",
    "    with Ctq(train_loader) as tq_train_loader:\n",
    "        for i, data in enumerate(tq_train_loader):\n",
    "            t_data = time.time() - iter_data_time\n",
    "            iter_start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            model.forward(data)\n",
    "            model.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tracker.track(model)\n",
    "\n",
    "            tq_train_loader.set_postfix(\n",
    "                **tracker.get_metrics(),\n",
    "                data_loading=float(t_data),\n",
    "                iteration=float(time.time() - iter_start_time),\n",
    "            )\n",
    "            iter_data_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
