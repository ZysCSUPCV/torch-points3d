{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nicolas-chaulet/torch-points3d/blob/modelnettuto/dashboard/ObjectClassificationRSConv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FFL6wvW8jvjO",
    "outputId": "e4618519-0953-4b1c-933b-d349a0d2eaa6"
   },
   "outputs": [],
   "source": [
    "# Setup packages\n",
    "!pip install torch==1.3.1 pyvista torchvision==0.4.2\n",
    "!pip install --upgrade jsonschema\n",
    "!pip install git+git://github.com/nicolas-chaulet/torch-points3d.git@modelnet\n",
    "!apt-get install -qq xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_eOTvwjjuK4"
   },
   "outputs": [],
   "source": [
    "# Needed for remote rendering \n",
    "import os\n",
    "os.system('export DISPLAY=:99.0 \\\n",
    "export PYVISTA_OFF_SCREEN=true \\\n",
    "export PYVISTA_USE_PANEL=true \\\n",
    "export PYVISTA_PLOT_THEME=document \\\n",
    "# This is needed for Panel - use with cuation!\\\n",
    "export PYVISTA_AUTO_CLOSE=false \\\n",
    "which Xvfb \\\n",
    "Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 & \\\n",
    "sleep 3 \\\n",
    "exec \"$@\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKtehn7D4drk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "import pyvista as pv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k14-WHdzDcvu"
   },
   "outputs": [],
   "source": [
    "DIR = \"\" # Replace with your root directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sS0YEUXjuLO"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"40%\" src=\"https://raw.githubusercontent.com/nicolas-chaulet/torch-points3d/master/docs/logo.png\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1cLXfkkjuLP"
   },
   "source": [
    "# Classifiying objects with Relation-Shape CNN\n",
    "This notebook explains how Torch Points3D can be used to solve an object classification task. We will use [ModelNet](https://modelnet.cs.princeton.edu/) as our dataset and [Relation-Shape CNN](https://github.com/Yochengliu/Relation-Shape-CNN) as our model architecture. The dataset contains CAD models of objects that belong to 40 different categories and the task is to retrieve the category of the object from its point cloud representation. There is also a smaller version well suited for quick testing that contains only 10 classes. The notebook covers the following aspects of Torch Points3D:\n",
    "\n",
    "1. Create a dataset with a data augmentation pipeline\n",
    "2. Instantiate a pre configured model\n",
    "3. Setup the data loaders\n",
    "4. Run a training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7O3JPuIQExZJ"
   },
   "outputs": [],
   "source": [
    "MODELNET_VERSION=\"40\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwOtGLfijuLR"
   },
   "source": [
    "## The dataset\n",
    "We will use Torch Points3D to download and create the dataset. It automatically downloads a pre sampled version of ModelNet (with 10,000 points per object) and the data will be stored under the `ROOT/data/modelnet` directory. Creating the raw dataset is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "P4iIhBJAjuLT",
    "outputId": "fc8250a3-55b4-43b8-aecd-75a307f387ef"
   },
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.classification.modelnet import SampledModelNet\n",
    "import torch_points3d.core.data_transform as T3D\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "dataroot = os.path.join(DIR, \"data/modelnet\")\n",
    "pre_transform = T.Compose([T.NormalizeScale(), T3D.GridSampling(0.02)])\n",
    "dataset = SampledModelNet(dataroot, name=MODELNET_VERSION, train=True, transform=None,\n",
    "                 pre_transform=pre_transform, pre_filter=None)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0cY35y1juLZ"
   },
   "source": [
    "Each sample is a CAD model of an object (bathtub, toilet, bed, etc...), the `pos` attribute contains the coordinates of points sampled on each face of the mesh while `norm` contains the normal vector. You will notice that we have applied two pre transforms to our dataset:\n",
    "\n",
    "- `NormalizeScale` normalises the scale of each object\n",
    "- `GridSampling` does a voxel downsizing of each point cloud with a resolution of 0.02 (in normalized scale). This ensures that we don't have really dense areas.\n",
    "\n",
    "Let's visualise some of those examples before going any further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "OsFH72B9juLa",
    "outputId": "649691f3-f621-4f7e-89fc-4f88d0679da0"
   },
   "outputs": [],
   "source": [
    "samples = [10,1000,3000]\n",
    "p = pv.Plotter(notebook=True,shape=(1, len(samples)),window_size=[1024,412])\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    p.subplot(0, i)\n",
    "    sample = dataset[samples[i]].pos.numpy()\n",
    "    point_cloud = pv.PolyData(sample)\n",
    "    point_cloud['y'] = sample[:,1]\n",
    "    p.add_points(point_cloud)\n",
    "    p.camera_position = [-1,5, -10]\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYE0OSUhjuLg"
   },
   "source": [
    "## Building a model\n",
    "As mentioned in the introduction we will use Relation-Shape CNN as our backbone architecture for the model classifier. Let's build our model by using Torch Points3D API (you can ignore the error, it comes from a third party library and all works just fine). We will use the normal vector as an input feature, therefore we will set the number of input channels (`input_nc`) to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWvt8XsojuLi"
   },
   "outputs": [],
   "source": [
    "from torch_points3d.applications.rsconv import RSConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffGE6xUNjuLo"
   },
   "outputs": [],
   "source": [
    "class RSConvCLassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.encoder = RSConv(\"encoder\", input_nc=3,output_nc = int(MODELNET_VERSION), num_layers=4)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    @property\n",
    "    def conv_type(self):\n",
    "        \"\"\" This is needed by the dataset to infer which batch collate should be used\"\"\"\n",
    "        return self.encoder.conv_type\n",
    "    \n",
    "    def get_output(self):\n",
    "        \"\"\" This is needed by the tracker to get access to the ouputs of the network\"\"\"\n",
    "        return self.output\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\" Needed by the tracker in order to access ground truth labels\"\"\"\n",
    "        return self.labels\n",
    "    \n",
    "    def get_current_losses(self):\n",
    "        \"\"\" Entry point for the tracker to grab the loss \"\"\"\n",
    "        return {\"loss_class\": float(self.loss_class)}\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Set labels for the tracker\n",
    "        self.labels = data.y.squeeze()\n",
    "\n",
    "        # Forward through the network\n",
    "        data_out = self.encoder(data)\n",
    "        self.output = self.log_softmax(data_out.x.squeeze())\n",
    "\n",
    "        # Set loss for the backward pass\n",
    "        self.loss_class = torch.nn.functional.nll_loss(self.output, self.labels)\n",
    "    \n",
    "    def backward(self):\n",
    "         self.loss_class.backward()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "r-C7fltXjuLt",
    "outputId": "d834c92b-cfde-4fe1-d77e-d524e87c5141"
   },
   "outputs": [],
   "source": [
    "model = RSConvCLassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S30iNr4TjuLx"
   },
   "source": [
    "## Setting up the data loaders\n",
    "In this section, we will first build  a data loader using basic building blocks, and then we will see how one can use our dataset wrapper to assemble a dataset ready for training with its test and train data loader configured.\n",
    "\n",
    "Let's start with a vanillia data loader. There are two ways to assemble batches depending on the model that is used. In that case we will want dense batches where samples are concatenated along the first dimension, just like for batches of images. It is very important to ensure that each sample has got the same number of points, this can be achieved by applying a `FixedPoints` transform to the dataset. here we will use 2048 points per object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UMKyO1qPjuLy",
    "outputId": "e14a865d-98dc-41db-a312-c6e94a290331"
   },
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.batch import SimpleBatch\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "transform = T.FixedPoints(2048)\n",
    "dataset = SampledModelNet(dataroot, name=MODELNET_VERSION, train=True, transform=transform,\n",
    "                 pre_transform=pre_transform, pre_filter=None)\n",
    "\n",
    "collate_function = lambda datalist: SimpleBatch.from_data_list(datalist)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,  \n",
    "    collate_fn=collate_function\n",
    ")\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FR1oib1ajuL4"
   },
   "source": [
    "If we were using a model that can handle packed data we would not need the `FixedPoints` transform at all and each object could have a different number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-L5aUdhjuL4"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "dataset = SampledModelNet(dataroot, name=MODELNET_VERSION, train=True, transform=None,\n",
    "                 pre_transform=pre_transform, pre_filter=None)\n",
    "\n",
    "collate_function = lambda datalist: Batch.from_data_list(datalist)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,  \n",
    "    collate_fn=collate_function\n",
    ")\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UkV3L28juL9"
   },
   "source": [
    "We also provide a higher level interface that builds the train and test data loaders for you based on which model you use. We use an OmegaConf config to instantiate it (or any object with a similar interface). The config must contain the arguments required by the dataset. We can also include the pre transforms, test transforms and train transforms. The transform pipeline will look like this:\n",
    "\n",
    "1. Sample a fixed number of points\n",
    "2. Add random noise to the positions\n",
    "3. Randomly rotate the point cloud around the z axis\n",
    "4. Add the normal vector to the features getting into the network\n",
    "\n",
    "The config will be as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmcefLM9juL-"
   },
   "outputs": [],
   "source": [
    "yaml_config = \"\"\"\n",
    "task: classification\n",
    "class: modelnet.ModelNetDataset\n",
    "name: modelnet\n",
    "dataroot: {}\n",
    "number: {}\n",
    "pre_transforms:\n",
    "    - transform: NormalizeScale\n",
    "    - transform: GridSampling\n",
    "      lparams: [0.02]\n",
    "train_transforms:\n",
    "    - transform: FixedPoints\n",
    "      lparams: [2048]\n",
    "    - transform: RandomNoise\n",
    "    - transform: RandomRotate\n",
    "      params:\n",
    "        degrees: 180\n",
    "        axis: 2\n",
    "    - transform: AddFeatsByKeys\n",
    "      params:\n",
    "        feat_names: [norm]\n",
    "        list_add_to_x: [True]\n",
    "        delete_feats: [True]\n",
    "test_transforms:\n",
    "    - transform: FixedPoints\n",
    "      lparams: [2048]\n",
    "    - transform: AddFeatsByKeys\n",
    "      params:\n",
    "        feat_names: [norm]\n",
    "        list_add_to_x: [True]\n",
    "        delete_feats: [True]\n",
    "\"\"\".format(os.path.join(DIR, \"data\"),MODELNET_VERSION)\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "params = OmegaConf.create(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "LkGJygG2juMD",
    "outputId": "23ccd73b-09c5-4253-b473-a98d48c186f6"
   },
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.classification.modelnet import ModelNetDataset\n",
    "dataset = ModelNetDataset(params)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z86lKgNejuMI"
   },
   "source": [
    "We can see here that the train and test transforms have been set properly. The batch size is still not set since the data loaders have not been instantiaded yet, let's do that now and we will have a dataset ready for training. We can also instantiate the tracker directly from the dataset which reduces potential risks in using the wrong metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfVbU3VHjuMJ"
   },
   "outputs": [],
   "source": [
    "dataset.create_dataloaders(\n",
    "    model, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    precompute_multi_scale=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YC0wA65jjuMN",
    "outputId": "203e270b-80e0-4715-8bac-c1342bee9709"
   },
   "outputs": [],
   "source": [
    "next(iter(dataset.test_dataloaders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oD9MKKlyjuMR",
    "outputId": "c82f1cb1-8252-4a5d-91e7-3e5082d97333"
   },
   "outputs": [],
   "source": [
    "tracker = dataset.get_tracker(False, False)\n",
    "tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cwz7oGwLjuMV"
   },
   "source": [
    "## Training!\n",
    "We can now start our training loop, we will use the Adam optimizer with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pTV-k6t4juMW"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Uso46P7juMa"
   },
   "outputs": [],
   "source": [
    "from torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq\n",
    "import time\n",
    "\n",
    "def train_epoch(device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    tracker.reset(\"train\")\n",
    "    train_loader = dataset.train_dataloader\n",
    "    iter_data_time = time.time()\n",
    "    with Ctq(train_loader) as tq_train_loader:\n",
    "        for i, data in enumerate(tq_train_loader):\n",
    "            t_data = time.time() - iter_data_time\n",
    "            iter_start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            data.to(device)\n",
    "            model.forward(data)\n",
    "            model.backward()\n",
    "            optimizer.step()\n",
    "            if i % 10 == 0:\n",
    "                tracker.track(model)\n",
    "\n",
    "            tq_train_loader.set_postfix(\n",
    "                **tracker.get_metrics(),\n",
    "                data_loading=float(t_data),\n",
    "                iteration=float(time.time() - iter_start_time),\n",
    "            )\n",
    "            iter_data_time = time.time()\n",
    "\n",
    "def test_epoch(device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    tracker.reset(\"test\")\n",
    "    test_loader = dataset.test_dataloaders[0]\n",
    "    iter_data_time = time.time()\n",
    "    with Ctq(test_loader) as tq_test_loader:\n",
    "        for i, data in enumerate(tq_test_loader):\n",
    "            t_data = time.time() - iter_data_time\n",
    "            iter_start_time = time.time()\n",
    "            data.to(device)\n",
    "            model.forward(data)           \n",
    "            tracker.track(model)\n",
    "\n",
    "            tq_test_loader.set_postfix(\n",
    "                **tracker.get_metrics(),\n",
    "                data_loading=float(t_data),\n",
    "                iteration=float(time.time() - iter_start_time),\n",
    "            )\n",
    "            iter_data_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "mfuxvTRbjuMe",
    "outputId": "ade434cd-7ae8-4a63-cec3-c1e3a348e0e8"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "for i in range(50):\n",
    "  print(\"=========== EPOCH %i ===========\" % i)\n",
    "  time.sleep(0.5)\n",
    "  train_epoch('cuda')\n",
    "  test_epoch('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atrcEWBW64uj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "ObjectClassificationRSConv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
